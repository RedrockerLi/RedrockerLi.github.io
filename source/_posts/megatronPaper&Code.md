---
title: Megatron:ä»æ€æƒ³åˆ°å®ç°
date: 2025-01-06 13:52:43
updated: 2025-02-11 10:04:36
tags:
    - MLsys
excerpt: paper&code
---
Megatron æ˜¯ç”± NVIDIA å¼€å‘çš„ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åˆ†å¸ƒå¼æ¡†æ¶ï¼Œç‰¹åˆ«é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## æ€æƒ³

### Model Parallel Transformers

å‡ºè‡ªã€ŠMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelismã€‹ï¼Œå®é™…æ˜¯å¼ é‡å¹¶è¡Œã€‚

#### é™ä½é€šä¿¡é‡çš„æ–¹æ³•

æˆ‘ä»¬ä»MLPå±‚å‡ºå‘ï¼Œè¡¨è¾¾å¼ä¸ºï¼š
$$
Y=GeLU(XA)
$$
æŠŠXæŒ‰åˆ—åˆ‡åˆ†ï¼ŒAæŒ‰è¡Œåˆ‡åˆ†ï¼š
$$
X=[X_1,X_2],A=[A_1,A_2]^T
$$
ä½†æ˜¯$GeLU$ä¸æ˜¯çº¿æ€§å‡½æ•°ï¼š
$$
Y=GeLU(X_1A_1+X_2A_2)\ne GeLU(X_1A_1)+GeLU(X_2A_2)
$$
è¿™æ—¶ï¼Œæˆ‘ä»¬éœ€è¦åœ¨$GeLU$å±‚å‰æ·»åŠ ä¸€ä¸ªåŒæ­¥ç‚¹æ¥åŒæ­¥$XA$çš„ç»“æœã€‚

ä½†ä¹Ÿå¯ä»¥æŠŠAæŒ‰åˆ—æ‹†åˆ†ï¼Œå¾—åˆ°ï¼š
$$
Y=[Y_1,Y_2]=[GeLU(XA_1),GeLU(XA_2)]
$$
è¿™æ ·å¯ä»¥èŠ‚çœä¸€æ¬¡åŒæ­¥æ“ä½œã€‚

åŒæ—¶ï¼Œè¾“å‡ºåµŒå…¥æ—¶ä¹Ÿå¯ä»¥é‡‡å–ä¸€æ ·çš„ç­–ç•¥æŠŠåŒæ­¥æ•°æ®é‡ä»$bsv$é™ä½åˆ°$bs$($b$æ˜¯bath-size,$s$æ˜¯sequence length,$v$æ˜¯vocabulary size).å¯¹äºå¤§æ¨¡å‹ä¸­è¾ƒå¤§çš„$v$ï¼Œè¿™æ ·åšçš„æ•ˆæœæ˜¯å¾ˆå¥½çš„ã€‚

![transformerå¼ é‡å¹¶è¡Œ](https://cdn.jsdelivr.net/gh/RedrockerLi/picx-images-hosting@master/MLsys/transformerå¼ é‡å¹¶è¡Œ.1e8oosklfk.webp)

ä¸Šé¢æ˜¯æ¨¡å‹å¹¶è¡Œå®ç°çš„æµç¨‹å›¾,$f,g$æ˜¯all-reduceæ“ä½œï¼Œåœ¨å‰å‘ä¼ é€’å’Œåå‘ä¼ é€’ä¸­å„éœ€è¦2æ¬¡ã€‚

#### åŒæ­¥æ•°æ®é‡

åœ¨MLPå’ŒSelf-Attentionä¸­ï¼Œä¸€æ¬¡é€šä¿¡çš„æ•°æ®é‡æ˜¯$bsh$ï¼ˆ$h$æ˜¯hidden dimension)

### pipline

å‡ºè‡ªã€ŠEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LMã€‹

åœ¨ä¸‹é¢çš„åˆ†æä¸­ï¼Œä¼šæŠŠæµæ°´çº¿å¹¶è¡Œå’Œç¡¬ä»¶è®¾è®¡ä¸­çš„æµæ°´çº¿å¯¹æ¯”ã€‚

#### pipline bubblesäº§ç”ŸåŸå› 

åœ¨ç¡¬ä»¶è®¾è®¡ä¸­ï¼Œæ— æ•ˆçš„æµæ°´çº§äº§ç”ŸåŸå› é€šå¸¸æ˜¯äº§ç”Ÿäº†æ•°æ®ç›¸å…³ï¼Œéœ€è¦é˜»å¡æŸä¸ªæµæ°´çº§ã€‚

åœ¨æµæ°´çº¿å¹¶è¡Œä¸­ï¼Œpipline bubblesä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯äº§ç”Ÿäº†æ•°æ®ç›¸å…³â€”â€”éœ€è¦ç­‰å¾…ä¸Šä¸€çº§å‰å‘ä¼ é€’å®Œæˆæˆ–ä¸‹ä¸€çº§åå‘ä¼ æ’­å®Œæˆã€‚

![æµæ°´çº¿å¹¶è¡Œ](https://cdn.jsdelivr.net/gh/RedrockerLi/picx-images-hosting@master/MLsys/æµæ°´çº¿å¹¶è¡Œ.4ckyscqdu4.webp)

è€Œä¸”è¿™ç§æ•°æ®ç›¸å…³æ˜¯å¿…ç„¶ä¼šå‘ç”Ÿï¼Œæ— æ³•è§„é¿çš„ã€‚è€Œä¸”æ¯ä¸€ä¸ªbatchå¼€å§‹ï¼Œç»“æŸéƒ½ä¼šäº§ç”Ÿå¡«å……ï¼Œå†²åˆ·æµæ°´çº¿å¸¦æ¥çš„ç©ºæ³¡ã€‚è¿™ä¸ªç©ºæ³¡çš„é—®é¢˜æ¯”è¾ƒå¥½è§£å†³ï¼Œä¸æ–­å¢å¤§batchå°±å¥½ï¼Œå¦‚æœä¸€è½®è®­ç»ƒåªæœ‰ä¸€æ¬¡æµæ°´çº¿ï¼Œé‚£å°±åªæœ‰ä¸€æ¬¡å¡«å……ï¼Œå†²åˆ·æµæ°´çº¿çš„å¼€é”€ã€‚ä½†è¿™æ˜¾ç„¶æ˜¯ä¸å¯èƒ½çš„ã€‚

æ‰€æœ‰å¯ä»¥æ›²çº¿æ•‘å›½ï¼ŒæŠŠmicrobatchåˆ‡å¾—æ›´å°ï¼Œæ¯ä¸€çº§è¿è¡Œçš„æ—¶é—´æ›´çŸ­ï¼Œå¡«å……ï¼Œå†²åˆ·æµæ°´çº¿çš„å¼€é”€å°±è¶Šå°ã€‚å½“ç„¶ï¼Œä¹Ÿä¸æ˜¯åˆ‡çš„è¶Šå°è¶Šå¥½ï¼Œå¤ªå°çš„çŸ©é˜µè¿ç®—åè€Œä¼šé™ä½æ€§èƒ½ã€‚

#### å¦‚ä½•æ‹†åˆ†microbatch

æˆ‘ä»¬å…ˆæŒ‰ç…§â€œåå‘ä¼ æ’­å¯ä»¥è¿›è¡Œå°±è¿›è¡Œï¼Œå‰å‘ä¼ æ’­ç”¨äºå¡«ç©ºâ€çš„æ€è·¯æ¥è®¾è®¡ä¸€ä¸‹è¿™ä¸ªåœ°æ–¹çš„æµæ°´çº¿ã€‚

| 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   | 19   | 20   | 21   | 22   | 23   | 24   | 25   | 26   | 27   | 28   | 29   | 30   | 31   | 32   | 33   | 34   | 35   | 36   | 37   | 38   | 39   | 40   | 41   | 42   | 43   | 44   | 45   | 46   | 47   | 48   | 49   | 50   | 51   | 52   | 53   | 54   | 55   | 56   | 57   | 58   | 59   | 60   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1a   | 2a   | 3a   | 4a   | 1    | 2    | 3    | 4    | 5a   | 6a   | 7a   | 8a   |      | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 5    | 6    | 7    | 8    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   |      |      |      |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |
|      | 1a   | 2a   | 3a   | 4a   | 1    | 2    | 3    | 4    | 5a   | 6a   | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 7a   | 8a   |      | 5    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 6    | 7    | 8    |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |
|      |      | 1a   | 2a   | 3a   | 4a   | 1    | 2    | 3    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 4    |      |      |      | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 5a   | 6a   | 7a   | 8a   | 5    | 6    | 7    | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   | 8    |      |      |      | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      |
|      |      |      | 1a   | 2a   | 3a   | 4a   | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | 1    | 2    | 3    | 4    | 1f   | 1f   | 2f   | 2f   | 3f   | 3f   | 4f   | 4f   | ï¼   | ï¼   | ï¼   | 5a   | 6a   | 7a   | 8a   | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   | 5    | 6    | 7    | 8    | 5f   | 5f   | 6f   | 6f   | 7f   | 7f   | 8f   | 8f   |      |      |      |      |      |      |

æœ‰12æ®µæ—¶é—´è¢«æµªè´¹ã€‚ä¸‹é¢åˆ†ææ€§èƒ½æŸå¤±çš„æ¥æºã€‚

ä¸€å®šä¼šè¢«æµªè´¹çš„åœ°æ–¹ï¼šæµæ°´çº¿å¡«å……å’Œå†²åˆ·ã€‚å¯¹äºæœ€åä¸€ä¸ªè®¾å¤‡ï¼Œå¡«å……å’Œå†²åˆ·çš„æ—¶é—´æµªè´¹åœ¨ä¸€å¤´ä¸€å°¾ï¼Œä¸­é—´æ²¡æœ‰æµªè´¹ã€‚ä½†ä¸Šé¢çš„æ„é€ åœ¨28-30çš„æ—¶é—´æµªè´¹åœ¨ç­‰å¾…5açš„å‰å‘ä¼ æ’­ã€‚ï¼ˆæ¯å°è®¾å¤‡è®¡ç®—æ—¶é—´ä¸€æ ·ï¼Œå¯ä»¥åªåˆ†æä¸€å°ï¼‰

æ‰€ä»¥çœŸæ­£çš„åŸåˆ™æ˜¯ï¼šâ€ä¸æµªè´¹é™¤äº†æµæ°´çº¿å¡«å……å’Œå†²åˆ·çš„æ—¶é—´â€œã€‚

ä¸‹é¢æ˜¯æ„é€ æ–¹æ³•ï¼šæœ€åä¸€å°è®¾å¤‡å¯ä»¥éšæ—¶å‘èµ·åå‘ä¼ æ’­ï¼Œæ‰€ä»¥è¦è®©æœ€åä¸€å°å°½å¯èƒ½å¿«é€Ÿçš„æ”¶åˆ°æ›´å¤šå‰å‘ä¼ æ’­çš„å€¼ã€‚åŒæ—¶é‡‡å–äº†äº¤å‰å‰ï¼Œåå‘ä¼ æ’­çš„æ–¹å¼å‡å°‘äº†ä¸­é—´æ¿€æ´»å€¼çš„ä¿å­˜æ—¶é—´ã€‚

![æµæ°´çº¿æ”¹è¿›](https://cdn.jsdelivr.net/gh/RedrockerLi/picx-images-hosting@master/MLsys/æµæ°´çº¿æ”¹è¿›.58hg7tj2dm.webp)

#### æµæ°´çº¿æ•ˆç‡

è®°è®¡ç®—æ—¶é—´ä¸º$t_{id}$,ç©ºæ³¡æ—¶é—´ä¸º$t_{pb}$,microbatchæ•°é‡ä¸º$m$,æµæ°´çº¿çº§æ•°ä¸º$p$ï¼Œåˆ™å¯¹ä¼˜åŒ–å‰çš„æµæ°´çº¿æœ‰ï¼š
$$
\frac{t_{pb}}{t_{id}}=\frac{p-1}{m}
$$
æŠŠä¸€ä¸ªmicrobatchåˆ†æˆ$v$ä»½ï¼Œåˆ™å¯¹ä¼˜åŒ–åçš„æµæ°´çº¿æœ‰ï¼š
$$
\frac{t_{pb}}{t_{id}}=\frac{1}{v} \frac{p-1}{m}
$$
è®ºæ–‡ä¸­çš„æè¿°æ˜¯ï¼š

> If each device has $ğ‘£$ stages (or model chunks), then the forward and backward time for a microbatch for each stage or chunk will now be $ğ‘¡_ğ‘“ /ğ‘£$ and $ğ‘¡_ğ‘ /ğ‘£$.

ç†è®ºä¸Šæ¥è¯´ï¼Œå¯ä»¥ä¸æŠŠè®¾å¤‡èµ„æºä¸¥æ ¼åˆ’åˆ†ä¸º$v$å—ï¼Œè®¾å¤‡å†…å­˜è¶³å¤Ÿä¿å­˜$v$æ¬¡å‰å‘ä¼ é€’ï¼Œä¸€æ¬¡åå‘ä¼ æ’­ï¼Œä¸€ä»½ä¸ºé€šä¿¡é¢„ç•™ï¼Œä¸€ä»½æ¨¡å‹å‚æ•°çš„ç©ºé—´å°±å¯ä»¥å®Œæˆä¸Šé¢çš„æµæ°´çº¿ï¼Œä½†æ–‡ç« ä¸­å¹¶æœªè®¨è®ºã€‚

#### åŒæ­¥æ•°æ®é‡

ä¸¤çº§ä¹‹é—´çš„é€šä¿¡é‡ä¸º$bsh$ã€‚åœ¨ä¸­é—´éƒ¨åˆ†ï¼Œå‰å‘å’Œåå‘äº¤æ›¿è¿›è¡Œæ—¶é€šä¿¡å’Œè®¡ç®—å¯ä»¥åŒæ—¶è¿›è¡Œã€‚

![piplineOverlap](https://cdn.jsdelivr.net/gh/RedrockerLi/picx-images-hosting@master/MLsys/piplineOverlap.32i1n3nuwo.webp)

### sequence parallelism

å‡ºè‡ªã€ŠReducing Activation Recomputation in Large Transformer Modelsã€‹

#### Activations Memory Per Transformer Layer

æ¯ä¸ª Transformer å±‚ç”±ä¸€ä¸ªæ³¨æ„åŠ›å—å’Œä¸€ä¸ª MLP å—ç»„æˆï¼Œå®ƒä»¬é€šè¿‡ä¸¤ä¸ªå±‚å½’ä¸€åŒ–è¿æ¥ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬æ¨å¯¼å­˜å‚¨æ¯ä¸ªè¿™äº›å…ƒç´ çš„æ¿€æ´»æ‰€éœ€çš„å†…å­˜ï¼š

- æŸ¥è¯¢ï¼ˆ$W^Q$ï¼‰ã€é”®ï¼ˆ$W^K$ï¼‰å’Œå€¼ï¼ˆ$W^V$ï¼‰çŸ©é˜µä¹˜æ³•ï¼šæˆ‘ä»¬åªéœ€è¦å­˜å‚¨å®ƒä»¬çš„å…±äº«è¾“å…¥ï¼Œå¤§å°ä¸º $2bsh$.
- QK^T çŸ©é˜µä¹˜æ³•ï¼šéœ€è¦å­˜å‚¨ Q å’Œ Kï¼Œæ€»å¤§å°ä¸º$4bsh$.
- Softmaxï¼šéœ€è¦ä¸€ä¸ªå¤§å°ä¸º $2as^2b$ çš„ Softmax è¾“å‡ºç”¨äºåå‘ä¼ æ’­.($a$:number of attention heads)
- Softmax dropoutï¼šåªéœ€è¦ä¸€ä¸ªå¤§å°ä¸º $as^2b$çš„æ©ç 
- å¯¹å€¼ï¼ˆVï¼‰çš„æ³¨æ„åŠ›ï¼šæˆ‘ä»¬éœ€è¦å­˜å‚¨ dropout è¾“å‡ºï¼ˆ$2as^2b$ï¼‰å’ŒV($2bsh$)
- ç»“æœå’Œ$W^O$çŸ©é˜µä¹˜ï¼šéœ€è¦ä¿å­˜å…¶è¾“å‡ºï¼Œå¤§å°ä¸º $2bsh$å­—èŠ‚
- dropoutï¼šéœ€è¦ä¿å­˜maskçŸ©é˜µï¼Œå¤§å°ä¸º $bsh$å­—èŠ‚
- MLPï¼šä¸¤ä¸ªçº¿æ€§å±‚å­˜å‚¨å®ƒä»¬çš„è¾“å…¥ï¼Œå¤§å°åˆ†åˆ«ä¸º $2bsh$ å’Œ $8bsh$ã€‚GeLU éçº¿æ€§å±‚ä¹Ÿéœ€è¦å…¶è¾“å…¥ï¼Œå¤§å°ä¸º $8bsh$ï¼Œç”¨äºåå‘ä¼ æ’­ã€‚æœ€åï¼Œdropout å­˜å‚¨å…¶æ©ç ï¼Œå¤§å°ä¸º $bsh$
- Layer normï¼šæ¯ä¸ªå±‚å½’ä¸€åŒ–å­˜å‚¨å…¶è¾“å…¥ï¼Œå¤§å°ä¸º $2bsh$ï¼Œæ€»å…±éœ€è¦ $4bsh$ çš„å­˜å‚¨ç©ºé—´

æ€»ç©ºé—´ä¸º$bsh(34+5\frac{as}{h})$

ä½¿ç”¨å¼ é‡å¹¶è¡Œåï¼š$bsh(10+\frac{24}{t}+5\frac{as}{ht})$ï¼ˆ$t$:tensor parallel sizeï¼‰

ä¸Šå¼ä¸­çš„$10$æ¥è‡ªï¼š

- QKVçŸ©é˜µä¹˜å…±äº«çš„å¤§å°ä¸º$2bsh$çš„è¾“å…¥
- è‡ªæ³¨æ„åŠ›dropoutçš„$bsh$çš„maskçŸ©é˜µ
- ç¬¬ä¸€ä¸ªçº¿æ€§å±‚è¾“å…¥éœ€è¦$2bsh$å­—èŠ‚
- MLP dropoutçš„$bsh$çš„maskçŸ©é˜µ
- Layer norm$4bsh$ çš„å­˜å‚¨ç©ºé—´

è¿™äº›å†…å­˜å ç”¨çš„åŸå› æ˜¯å¯¹transformerå—æ¥è¯´ï¼Œè¢«åˆ‡åˆ†åˆ°ä¸åŒGPUè¿ç®—æ—¶ï¼Œä¸Šé¢çš„å€¼éƒ½è¦å¤åˆ¶å¤šä»½ï¼Œæ‰€ä»¥åº”è¯¥æ€è€ƒæ€ä¹ˆæŠŠè¿™äº›éƒ¨åˆ†æ‹†åˆ°ä¸åŒçš„GPUä¸Šã€‚

#### æ‹†åˆ†æ–¹æ³•

åŒæ ·ä»¥MLPå±‚ä¸ºä¾‹ï¼Œæ‹†åˆ†æ–¹æ³•å¦‚ä¸‹å›¾ï¼š

![sequenceParallelis](https://cdn.jsdelivr.net/gh/RedrockerLi/picx-images-hosting@master/MLsys/sequenceParallelis.2vets7nwx4.webp)

MLP dropoutï¼ŒLayer normæ˜¯æ€ä¹ˆèŠ‚çœç©ºé—´çš„å¾ˆå¥½è§£é‡Šï¼ŒæŒ‰åºåˆ—çš„ç»´åº¦åˆ’åˆ†ã€‚ä½†ä¸¤ä¸ªè¾“å…¥ä¼¼ä¹è¿˜æ˜¯è¦å¤åˆ¶å¤šä»½ã€‚è¿™é‡Œé‡‡ç”¨çš„ç­–ç•¥æ˜¯ï¼šä¸å‚¨å­˜å®Œæ•´çš„Yæ‹¥æœ‰åå‘ä¼ æ’­ï¼Œåœ¨ç¬¬iä¸ªå¼ é‡å¹¶è¡Œè®¾å¤‡ä¸Šä¿å­˜$Y_i^s$éƒ¨åˆ†ï¼Œåœ¨åå‘ä¼ æ’­æ˜¯æ‰§è¡Œé¢å¤–çš„all-gatherã€‚åŒæ—¶æŠŠé€šä¿¡å’Œè®¡ç®—å¯¹Yçš„æ¢¯åº¦å¹¶è¡Œï¼Œå‡å°‘å»¶è¿Ÿã€‚

å†…å­˜æ¶ˆè€—å˜ä¸º$bsh(\frac{34}{t}+5\frac{as}{ht})$

åŒæ—¶ï¼Œè¿™ç§ç­–ç•¥å’Œå¼ é‡å¹¶è¡Œæ¯”èµ·æ¥é€šä¿¡é‡ä¹Ÿæ²¡æœ‰ä¸Šå‡ã€‚å¼ é‡å¹¶è¡Œéœ€è¦all-reduceï¼Œå¼€é”€å’ŒåŠ ä¸Šåºåˆ—å¹¶è¡Œçš„reduce-scatter+all-gatherç›¸ç­‰ã€‚

### selective activation recomputation

å¦‚æœæ¯å±‚æ”¾å·²ç»æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥æŠŠå†…å­˜æ¶ˆè€—é™ä½ä¸º$2bsh$ï¼Œä½†éœ€è¦è®¡ç®—ä¸¤æ¬¡å‰å‘ä¼ é€’ï¼Œä¹Ÿæœ‰é¢å¤–çš„é€šä¿¡å¼€é”€ï¼Œä¸è€ƒè™‘ã€‚

ä¸ºäº†å¹³è¡¡å‚¨å­˜å’Œè¿ç®—çš„å¼€é”€ï¼Œæˆ‘ä»¬é€‰æ‹©å æœ‰è¾ƒå¤§å†…å­˜ï¼Œè®¡ç®—é‡ä¸å¤§çš„$\frac{5as}{h}$éƒ¨åˆ†ï¼Œå½“æ¨¡å‹å¾ˆå¤§æ—¶ï¼Œå®ƒä¼šè¶…è¿‡å‰é¢çš„34ã€‚åŒæ—¶é‡è®¡ç®—çš„æˆæœ¬ä¹Ÿè¾ƒä½ã€‚

ç†è®ºè®¡ç®—é‡ä¸ºï¼š
$$
72Bslh^2(1+\frac{s}{6h}+\frac{V}{12lh})
$$
å®é™…è®¡ç®—é‡:
$$
72Bslh^2(1+\frac{s}{3h}+\frac{V}{12lh})
$$


## å®ç°

ä»£ç ç‰ˆæœ¬æ˜¯tag: NVLM-1.0

### ç¨‹åºå…¥å£

#### é…ç½®æ–‡ä»¶

åœ¨`examples/gpt3/train_gpt3_175b_distributed.sh`æ–‡ä»¶ä¸­å±•ç¤ºäº†è®­ç»ƒè„šæœ¬çš„è¿è¡Œè¿‡ç¨‹ï¼Œæ¯”è¾ƒé‡è¦çš„å‚æ•°æœ‰ï¼š

```shell
GPUS_PER_NODE=8
MASTER_ADDR=localhost
MASTER_PORT=6000
NUM_NODES=1
NODE_RANK=0
```

æœ€åä½¿ç”¨torchrunæ¥è¿è¡Œï¼š

```shell
torchrun ${DISTRIBUTED_ARGS[@]} pretrain_gpt.py \ 
    ${GPT_MODEL_ARGS[@]} \
    ${TRAINING_ARGS[@]} \
    ${MODEL_PARALLEL_ARGS[@]} \
    ${DATA_ARGS[@]} \
    ${EVAL_AND_LOGGING_ARGS[@]}
```

ä¸‹é¢æ˜¯ä¸€ä¸ªè„šæœ¬ç¤ºä¾‹ï¼š

```shell
#!/bin/bash
module purge
module load compilers/cuda/12.1
module load nccl/2.18.3-1_cuda12.1
module load cudnn/8.9.5.29_cuda12.x
module load compilers/gcc/9.3.0
module load miniforge3/24.1

source activate Megatron_RR

export PYTHONUNBUFFERED=1
export TRANSFORMERS_VERBOSITY="debug"
export PYTHONPATH=/home/bingxing2/home/scx9ktr/LiChengyv/Megatron-LM:$PYTHONPATH

### å¯ç”¨IBé€šä¿¡
export NCCL_ALGO=Ring
export NCCL_MAX_NCHANNELS=16
export NCCL_MIN_NCHANNELS=16
export NCCL_DEBUG=INFO
export NCCL_TOPO_FILE=/home/bingxing2/apps/nccl/conf/dump.xml
export NCCL_IB_HCA=mlx5_0,mlx5_2
export NCCL_IB_GID_INDEX=3
export NCCL_IB_TIMEOUT=23
export NCCL_IB_RETRY_CNT=7

export CUDA_DEVICE_MAX_CONNECTIONS=1

export OMP_NUM_THREADS=8

HOSTFILE=/home/bingxing2/home/scx9ktr/LiChengyv/Megatron-LM/examples/gpt3/hostfile.log
echo > $HOSTFILE
### è·å–æ¯ä¸ªèŠ‚ç‚¹çš„ hostname
# for i in `srun hostname -s`
for i in `scontrol show hostnames`
do
  let k=k+1
  host[$k]=$i
  rank[$k]=$(($k-1))
  echo "${host[$k]} slots=$GPUS" >> $HOSTFILE
done

### è®¾ç½®ä¸»èŠ‚ç‚¹,å°†ç¬¬â¼€ä¸ªèŠ‚ç‚¹ä¸»æœºååšä¸º master åœ°å€.
MASTER_ADDR=${host[1]}
### Nodes
NODES="${#host[@]}"

export CUDA_DEVICE_MAX_CONNECTIONS=1
LR=1e-4

##Config nnodes node_rank master_addr
GPUS_PER_NODE=4
# WORLD_SIZE=$(($GPUS_PER_NODE*$NUM_NODES))


# 32B
MODEL_SIZE=32
GPT_MODEL_ARGS=(
    --num-layers 48 
    --hidden-size 1024 
    --num-attention-heads 16 
    --seq-length 1024 
    --max-position-embeddings 1024 
)
# LOG Dir And Path
output_home="./model"
log_path="${output_home}/log/"
checkpoint_path="${output_home}/output"
tensorboard_dir="${output_home}/output"
tensorboard_path="${tensorboard_dir}"




DATA_HOME="~/dataset/gpt_sample_dataset_text_document"
DATA_PATH="${DATA_HOME}/gpt_sample_dataset_text_document" #<Specify path and file prefix>_text_document
VOCAB_FILE="${DATA_HOME}/gpt2-vocab.json"
MERGE_FILE="${DATA_HOME}/gpt2-merges.txt"

TRAINING_ARGS=(
    --micro-batch-size 1 
    --global-batch-size 1024 
    --train-iters 500000 
    --weight-decay 0.1 
    --adam-beta1 0.9 
    --adam-beta2 0.95 
    --init-method-std 0.006 
    --clip-grad 1.0 
    --fp16
    --lr 6.0e-5 
    --lr-decay-style cosine 
    --min-lr 6.0e-6
    --lr-warmup-fraction .001 
    --lr-decay-iters 430000 
)

MODEL_PARALLEL_ARGS=(
        --tensor-model-parallel-size 2
        --pipeline-model-parallel-size 4 
)

DATA_ARGS=(
    --data-path $DATA_PATH 
    --vocab-file $VOCAB_FILE 
    --merge-file $MERGE_FILE 
    --split 949,50,1
)

EVAL_AND_LOGGING_ARGS=(
    --log-interval 2
    --save-interval 10000 
    --eval-interval 1000 
    --save $checkpoint_path 
    --load $checkpoint_path
    --eval-iters 100
    --tensorboard-dir $tensorboard_path
)


echo "torchrun --nproc_per_node=$GPUS_PER_NODE --master_port=29546 --nnodes=$NODES --node_rank=0 --master_addr="${host[1]}" \
    /home/bingxing2/home/scx9ktr/LiChengyv/Megatron-LM/pretrain_gpt.py \
    ${GPT_MODEL_ARGS[@]} \
    ${TRAINING_ARGS[@]} \
    ${MODEL_PARALLEL_ARGS[@]} \
    ${DATA_ARGS[@]} \
    ${EVAL_AND_LOGGING_ARGS[@]} >> ${SLURM_JOB_ID}.default_0.out 2>&1 &"

### ä¸»èŠ‚ç‚¹è¿è¡Œ
torchrun --nproc_per_node=$GPUS_PER_NODE --master_port=29546 --nnodes=$NODES --node_rank=0 --master_addr="${host[1]}" \
    /home/bingxing2/home/scx9ktr/LiChengyv/Megatron-LM/pretrain_gpt.py \
    ${GPT_MODEL_ARGS[@]} \
    ${TRAINING_ARGS[@]} \
    ${MODEL_PARALLEL_ARGS[@]} \
    ${DATA_ARGS[@]} \
    ${EVAL_AND_LOGGING_ARGS[@]} >> ${SLURM_JOB_ID}.default_0.out 2>&1 &

### ä½¿ç”¨ srun è¿è¡Œç¬¬äºŒä¸ªèŠ‚ç‚¹
for((i=2;i<=${NODES};i++));
do
    node_host=${host[$i]}
    node_rank=${rank[$i]}
    echo "nodes:${NODES}, host:${node_host}, node_rank:${node_rank},master_addr:${MASTER_ADDR}"
    srun -N 1 -w $node_host \
        /home/bingxing2/home/scx9ktr/LiChengyv/Megatron-LM/examples/gpt3/init.sh
    srun -N 1 --gres=gpu:$GPUS_PER_NODE -w $node_host \
        torchrun --nproc_per_node=$GPUS_PER_NODE --master_port=29546 --nnodes=$NODES --node_rank=$node_rank --master_addr="${MASTER_ADDR}" \
    /home/bingxing2/home/scx9ktr/LiChengyv/Megatron-LM/pretrain_gpt.py \
    ${GPT_MODEL_ARGS[@]} \
    ${TRAINING_ARGS[@]} \
    ${MODEL_PARALLEL_ARGS[@]} \
    ${DATA_ARGS[@]} \
    ${EVAL_AND_LOGGING_ARGS[@]} >> ${SLURM_JOB_ID}.default_${node_rank}.out 2>&1 &
done
wait
```

åŒæ—¶è¿˜æœ‰ä¸€äº›å‚æ•°é…ç½®åœ¨`examples/gpt3/gpt_config.yaml`ï¼Œåœ¨`parse_argsï¼ˆï¼‰`å‡½æ•°ä¸­è¢«è¯»å–ã€‚äº«ç”¨yamlæ–‡ä»¶éœ€è¦åŠ ä¸Š--yaml_cfgè¿™ä¸ªå‘½ä»¤è¡Œå‚æ•°ã€‚

#### pretrain_gpt.py

```python
if __name__ == "__main__":
    # Temporary for transition to core datasets
    train_valid_test_datasets_provider.is_distributed = True
    pretrain(
        train_valid_test_datasets_provider, # ç”¨äºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        model_provider,                     # ç”Ÿæˆæ¨¡å‹
        ModelType.encoder_or_decoder,       # æ­£åœ¨è¢«è®­ç»ƒçš„ç±»å‹,åˆ¤æ–­æ˜¯å¦è¦é’ˆå¯¹æ€§ä¼˜åŒ–
        forward_step,                       # å‰å‘ä¼ æ’­å‡½æ•°
        args_defaults={'tokenizer_type': 'GPT2BPETokenizer'},
    )
```

### 3Då¹¶è¡Œ

#### åˆå§‹åŒ–

pretrainä¸­è°ƒç”¨çš„ç¬¬ä¸€ä¸ªå‡½æ•°æ˜¯`initialize_megatron`ã€‚

```python
initialize_megatron(
    extra_args_provider=extra_args_provider,  # None
    args_defaults=args_defaults,		      # {'tokenizer_type': 'GPT2BPETokenizer'}
    get_embedding_ranks=get_embedding_ranks,  # None
    get_position_embedding_ranks=get_position_embedding_ranks # None
)
```

æ ¸å¿ƒåŠŸèƒ½æ˜¯`initialize_model_parallel`æä¾›çš„ã€‚

```python
mpu.initialize_model_parallel(
    # å¼ é‡å¹¶è¡Œçš„å¤§å°
    args.tensor_model_parallel_size, 
    # æµæ°´çº¿å¹¶è¡Œçš„å¤§å°
    args.pipeline_model_parallel_size, 
    # è™šæ‹Ÿæµæ°´çº¿å¹¶è¡Œçš„å¤§å°
    args.virtual_pipeline_model_parallel_size,
    # decoderçš„èµ·å§‹rank
    args.pipeline_model_parallel_split_rank,
    # ...
)
```

é¦–å…ˆåˆ›å»º`RankGenerator`å¯¹è±¡ï¼Œåˆå§‹åŒ–å¹¶è¡Œæ¨¡å¼çš„æ­£ç¡®é…ç½®å’Œé¡ºåºã€‚

```python
encoder_rank_generator = RankGenerator(
    tp=encoder_tensor_model_parallel_size,
    ep=1,
    dp=data_parallel_size,
    pp=encoder_pipeline_model_parallel_size,
    cp=context_parallel_size,
    order=order,
    rank_offset=0,
)
 decoder_rank_generator = RankGenerator(
    tp=tensor_model_parallel_size,
    ep=1,
    dp=data_parallel_size,
    pp=pipeline_model_parallel_size,
    cp=context_parallel_size,
    order=order,
    rank_offset=encoder_world_size,
)
```

é‡è¦æ–¹æ³•ï¼š

```python
def get_ranks(self, token):
    # tokenæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ¯”å¦‚â€œtp-dpâ€ï¼Œè¿”å›çš„maskæ˜¯ä¸€ä¸ªå’Œself.orderä¸€æ ·å¤§çš„å¸ƒå°”æ•°ç»„ã€‚tokenä¸­çš„å¹¶è¡Œç±»å‹åœ¨self.orderä¸­å‡ºç°ï¼Œåˆ™å¯¹åº”ä½ç½®çš„maskä¸ºtrueï¼Œåä¹‹ä¸ºfalse
    mask = self.get_mask(self.order, token)
    ranks = generate_masked_orthogonal_rank_groups(self.world_size, self.ordered_size, mask)
    if self.rank_offset > 0:
        for rank_group in ranks:
            for i in range(len(rank_group)):
                rank_group[i] += self.rank_offset
    return ranks
```

`generate_masked_orthogonal_rank_groups()`å‡½æ•°ç”¨äºåˆ’åˆ†è¿›ç¨‹ç»„ã€‚ä¼šæ ¹æ®å¹¶è¡Œæ–¹æ³•çš„å¹¶è¡Œåº¦å’Œtokenä¸­å‡ºç°çš„å¹¶è¡Œæ–¹æ³•æ¥ç”Ÿæˆã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

å‡è®¾`self.ordered_size`æ˜¯`[tp_size, pp_size, dp_size]`ï¼Œ`mask`æ˜¯`[True, False , True]`ã€‚

```
masked_shape=[tp_size, dp_size]
unmasked_shape=[pp_size]
global_stride=[1,tp_size,tp_size*pp_size,tp_size*pp_size*dp_size]
masked_stride=[1,tp_size*pp_size]
unmasked_stride=[tp_size]
group_size=tp_size*dp_size
num_of_group=world_size//(group_size)
å¯¹æ¯ä¸€ä¸ªç»„ï¼ˆfor group_index in range(num_of_group)ï¼‰
    è§£æ–¹ç¨‹decomposed_group_idx*unmasked_shape=group_index1
    å¯¹ç»„ä¸­çš„æ¯ä¸€ä¸ªæˆå‘˜ï¼ˆ for rank_in_group in range(group_size)ï¼‰
        è§£æ–¹ç¨‹decomposed_rank_idx*masked_shape=rank_in_group
        åœ¨rankä¸­æ·»åŠ decomposed_rank_idx*masked_stride+decomposed_group_idx*unmasked_stride
  	åœ¨ranksä¸­æ·»åŠ rank
```

è¿™æ ·åšå¯ä»¥ä¿è¯ä¸¤ç‚¹ï¼š

- æ‰€æœ‰è¿›ç¨‹çš„rankä¸é‡å¤ï¼Œå’Œ3Då¹¶è¡Œä¸­çš„ä½ç½®ä¸€ä¸€å¯¹åº”ï¼Œä¸”ä¸éšmaskçš„å˜åŒ–è€Œå˜åŒ–ã€‚
- å¯ä»¥æŒ‰å¹¶è¡Œæ–¹å¼ç»™è¿›ç¨‹åˆ†ç»„ï¼ŒæŠŠä¸€ç»„æ”¾åœ¨ä¸€ä¸ªå­åˆ—è¡¨é‡Œé¢ï¼Œå­åˆ—è¡¨ä¸ªæ•°æ˜¯unmaskedçš„sizeä¹˜ï¼Œè¡¨ç¤ºæŒ‰unmaskedæ¥åˆ’åˆ†å­åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨å†…éƒ½æ˜¯åœ¨maskedçš„å¹¶è¡Œæ–¹æ³•ä¸­å¤„äºåŒä¸€ä½ç½®çš„rankã€‚

`generator_wrapper`å‡½æ•°ï¼š

```python
for ranks in generator_wrapper('dp'):
    # åˆ›å»ºä¸¤ä¸ªä½¿ç”¨ä¸åŒé€šä¿¡åç«¯çš„è¿›ç¨‹ç»„
    group = torch.distributed.new_group(
        ranks, timeout=timeout, pg_options=get_nccl_options('dp', nccl_comm_cfgs)
    )
    group_gloo = torch.distributed.new_group(ranks, timeout=timeout, backend="gloo")
    if rank in ranks:
        _DATA_PARALLEL_GROUP = group
        _DATA_PARALLEL_GROUP_GLOO = group_gloo
        _DATA_PARALLEL_GLOBAL_RANKS = ranks
```

è¿™é‡Œdpæ–¹æ³•ä¼šè¿”å›å¤šä¸ªå­åˆ—è¡¨ï¼Œæ¯ä¸€ä¸ªå­åˆ—è¡¨rankå¤„åœ¨dpçš„åŒä¸€ä½ç½®ï¼Œä¼šäº¤æ¢æ•°æ®ï¼Œæ‰€ä»¥åŠ å…¥åŒä¸€ä¸ªé€šä¿¡ç»„ã€‚

å¦‚æœè¾“å…¥å‚æ•°æ˜¯pp,åˆ™ä¼šè¿”å›encoderå’Œdecoderå¤„åœ¨åŒä¸€ä½ç½®çš„ç»„ï¼Œä¸”encoderå¯ä»¥å¾ªç¯ä½¿ç”¨ã€‚

å¦‚æœè¾“å…¥å‚æ•°æ˜¯tp-pp,åˆ™ä¼šè¿”å›encoderå’Œdecoderå¤„åœ¨åŒä¸€ä½ç½®çš„ç»„ï¼Œä¸”encoderä¸å¯ä»¥å¾ªç¯ä½¿ç”¨ï¼Œencoderå’Œdecoderå¿…é¡»ä¸€æ ·é•¿ã€‚

è®¾ç½®åŒä¸€ä½ç½®çš„ä¸ºä¸€ç»„ï¼Œæ˜¯ä¿è¯å¤„åœ¨åŒä¸€ä½ç½®çš„å¯ä»¥å®ç°é›†åˆé€šä¿¡ï¼ˆå¦‚AllReduceï¼‰ã€‚å¯¹äºé˜¶æ®µé—´é€šä¿¡ï¼Œå¯ä»¥ä½¿ç”¨ç‚¹å¯¹ç‚¹é€šä¿¡ã€‚

åŒæ—¶ä½¿ç”¨`get_model`å‡½æ•°åˆ†å‰²æ¨¡å‹ï¼Œä¸»è¦æ˜¯æŒ‰ç…§æµæ°´çº§åˆ’åˆ†ï¼ŒåŒæ—¶ä¼šè®¾ç½®æ•°æ®å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œçš„å‚æ•°ã€‚

#### è®­ç»ƒ

é‡è¦çš„å‡½æ•°æ˜¯`forward_backward_pipelining_with_interleaving`ï¼Œåˆ†å—å®ç°æµæ°´çº¿çš„å¡«å……ï¼Œæ­£å¸¸è¿è¡Œï¼Œå†²åˆ·ã€‚

ä½¿ç”¨ä¸€ä¸ªä¾‹å­èµ°ä¸€éå‡½æ•°æµç¨‹ï¼š

PP3 N3M5 with VP2

```python
pipeline_parallel_size = 3
num_microbatches = 5
num_model_chunks = 3
total_num_microbatches = 15
config.microbatch_group_size_per_vp_stage
```

åœ¨å‰å‘ä¼ æ’­+åå‘ä¼ æ’­çš„æƒ…å†µä¸‹ï¼š

```python
num_warmup_microbatches = (pipeline_parallel_size - pipeline_parallel_rank - 1) * 2
num_warmup_microbatches+= (num_model_chunks - 1)*config.microbatch_group_size_per_vp_stage
# num_warmup_microbatches = min(8-2x,total_num_microbatches) 
```

ä¸‹é¢çœ‹çœ‹xçš„å–å€¼ï¼š

ç±»ä¼¼ä¸Šé¢çš„dp,_PIPELINE_MODEL_PARALLEL_GROUPä¼šè¢«ç”Ÿæˆä¸ºä¸€ä¸ªåˆ—è¡¨ï¼ˆå¦‚æœæœ‰å¤šä¸ªgroup,å› ä¸ºgenerator_wrapperä¸­å¦‚æœæ˜¯pp,encoderå¯ä»¥å¾ªç¯ä½¿ç”¨ï¼‰ã€‚ä¸”å¦‚æœä¸€ä¸ªrankåœ¨å¤šä¸ªç»„ä¸­ï¼Œä»–ä»¬æœ‰ç›¸åŒçš„index,æ‰€ä»¥çœ‹pp_group[0]çš„rankå³å¯ã€‚

| virtual_microbatch_id | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   |
| --------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| microbatch_id         | 0    | 1    | 2    | 0    | 1    | 2    | 0    | 1    | 2    | 3    | 4    | 3    | 4    | 3    | 4    |
| model_chunk_id        | 0    | 0    | 0    | 1    | 1    | 1    | 2    | 2    | 2    | 0    | 0    | 1    | 1    | 2    | 2    |

å¦‚æœæ˜¯æµæ°´çº¿çš„ç¬¬äºŒçº§ï¼Œå½“k=1æ—¶ï¼Œrecv_tensor_from_previous_stageè¿”å›å€¼ä¸ºTrue,0

```python
recv_prev = True
next_forward_model_chunk_id = 0
fwd_recv_buffer[1] = 
```

> æµæ°´çº¿å¹¶è¡Œé€šä¿¡ï¼š`p2p_communication.send_forward_recv_forward`->`_communicate`
>
> ```python
> return tensor_recv_prev, tensor_recv_next, reqs
> # tensor_recv_prev : tensor or Noneï¼Œå–å†³äºrecv_prevæ˜¯å¦ä¸ºTrue
> # reqs : ä¸€ç³»åˆ—é€šä¿¡è¯·æ±‚
> ```
>
> `initialize_model_parallel`æ—¶åœ¨ä½¿ç”¨`generator_wrapper('pp')`åè®¾ç½®äº†`_PIPELINE_MODEL_PARALLEL_GROUP`ï¼Œè®¾ç½®æ—¶ä¸ºå½“å‰rankæ‰€åœ¨ç»„ã€‚åœ¨è¿™é‡Œé€šä¿¡çš„æ—¶å€™æŒ‡å®šè¯¥ç»„ã€‚

æ•°æ®å¹¶è¡Œæ˜¯åœ¨åˆ’åˆ†æ•°æ®é›†ï¼Œæä¾›è®­ç»ƒæ•°æ®æ—¶å®ç°çš„;å¼ é‡å¹¶è¡Œæ˜¯åœ¨å®šä¹‰æ¨¡å‹æ—¶å®ç°çš„ã€‚



### selective activation recomputation

åœ¨`class CheckpointFunction(torch.autograd.Function):`ä¸­å®ç°ã€‚

```python
with torch.no_grad():
    outputs = run_function(*args)
if distribute_saved_activations:
    ctx.input_0_shape = args[0].data.shape
    safely_set_viewless_tensor_data(
        args[0], split_tensor_into_1d_equal_chunks(args[0].data, new_buffer=True)
    )
ctx.save_for_backward(*args)
```

åœ¨å®šä¹‰attentionæ¨¡å—æ—¶ä½¿ç”¨ï¼š

```python
hidden_states = tensor_parallel.checkpoint(
        custom_forward, False, query, key, value,
        attention_mask, rotary_pos_emb, attn_mask_type
    )
```

> å‚è€ƒæ–‡çŒ®ï¼š
>
> [1] [ä¸‡å­—é•¿æ–‡è§£æï¼šå¤§æ¨¡å‹éœ€è¦æ€æ ·çš„ç¡¬ä»¶ç®—åŠ›](https://mp.weixin.qq.com/s/NoREsyLXNVk1aABtSkhBDA)
>
> [2]Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
>
> [3]Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
>
> [4]Reducing Activation Recomputation in Large Transformer Models